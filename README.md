# CMPE 133 Political Analyzer

## Introduction
This is our Group Project for CMPE 133. It's a full stack React.js app rolled using Next.js. Our backend is running on python using FastAPI.

Our project is intended to help people more easily understand the angle of their news content. Using machine learning techniques, our web app will return the political leaning of an article ranging from left (1) to right (0)

## Quickstart 

### Requirements to run
Make sure to have python v3.12+ and Node.js v18+ installed and linked through environmental variables

### Fastapi-SQLite
NOTE: While in the Fastapi-SQLite directory in a terminal window, create a binaries folder and store the mlp_model.joblib file (which can be found [here](https://drive.google.com/drive/folders/1X3ZzlYjBjqDRfIfiwIEuNOo-chdCSpYI?usp=sharing)) in it.
<br>

1. Install Dependencies.
```
pip install -r requirements.txt
```
2. Create a venv (if not present) and activate it (instructions for unix/unix-like operating systems)
```
python -m venv venv 

source venv/bin/activate
```

3. Use the provided makefile (assuming you have make installed) to spin up the dev server
```
make dev
//  Otherwise use the following command
uvicorn main:app --reload
```
4. Test endpoints with cURL/Tool of your choosing(e.g. Postman/Insomnia) OR just integrate to frontend.

<br>

Port number is 8000, so the full baseURL will be http://localhost:8000

Autogenerated swagger docs are on http://localhost:8000/docs#/

### political-analyzer
1. Install Dependencies.
```
cd political-analyzer
npm install --force
```
2. Spin up the dev server
```
npm run dev 
```
3. Open the site - The link is `http://localhost:3000`

### Note: Both the frontend and backend need to run simultaneously

### While running
When running the project for the first time, you will most likely have to download the large language model required for word embedding. This will take a few minutes to download and can be only be invoked when running the article analysis. 

## Contributions
Ryan: Worked on the frontend logic and integrated backend endpoints
<br>
Josh: Worked on the neural network model and scraper for parsing articles
<br>
Marcus: Worked on establishing the database and endpoints
<br>
Ngoc: Worked on frontend design
